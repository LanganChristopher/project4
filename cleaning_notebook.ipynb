{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_2010_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2010.csv')\n",
    "tide_2011_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2011.csv')\n",
    "tide_2012_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2012.csv')\n",
    "tide_2013_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2013.csv')\n",
    "tide_2014_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2014.csv')\n",
    "tide_2015_atlantic = pd.read_csv('./data/Atlantic_city_data/tide_levels/tide_2015.csv')\n",
    "tide_2010_boston = pd.read_csv('./data/Boston/tide_levels/tide_2010.csv')\n",
    "tide_2011_boston = pd.read_csv('./data/Boston/tide_levels/tide_2011.csv')\n",
    "tide_2012_boston = pd.read_csv('./data/Boston/tide_levels/tide_2012.csv')\n",
    "tide_2013_boston = pd.read_csv('./data/Boston/tide_levels/tide_2013.csv')\n",
    "tide_2014_boston = pd.read_csv('./data/Boston/tide_levels/tide_2014.csv')\n",
    "tide_2015_boston = pd.read_csv('./data/Boston/tide_levels/tide_2015.csv')\n",
    "tide_2010_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2010.csv')\n",
    "tide_2011_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2011.csv')\n",
    "tide_2012_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2012.csv')\n",
    "tide_2013_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2013.csv')\n",
    "tide_2014_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2014.csv')\n",
    "tide_2015_bridgeport = pd.read_csv('./data/bridgeport_data/tide_levels/tide_2015.csv')\n",
    "tide_2010_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2010.csv')\n",
    "tide_2011_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2011.csv')\n",
    "tide_2012_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2012.csv')\n",
    "tide_2013_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2013.csv')\n",
    "tide_2014_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2014.csv')\n",
    "tide_2015_new_haven = pd.read_csv('./data/new_haven_data/tide_levels/tide_2015.csv')\n",
    "tide_2010_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2010.csv')\n",
    "tide_2011_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2011.csv')\n",
    "tide_2012_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2012.csv')\n",
    "tide_2013_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2013.csv')\n",
    "tide_2014_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2014.csv')\n",
    "tide_2015_new_london = pd.read_csv('./data/new_london_data/tide_levels/tide_2015.csv')\n",
    "tide_2010_newport = pd.read_csv('./data/newport/tide_levels/tide_2010.csv')\n",
    "tide_2011_newport = pd.read_csv('./data/newport/tide_levels/tide_2011.csv')\n",
    "tide_2012_newport = pd.read_csv('./data/newport/tide_levels/tide_2012.csv')\n",
    "tide_2013_newport = pd.read_csv('./data/newport/tide_levels/tide_2013.csv')\n",
    "tide_2014_newport = pd.read_csv('./data/newport/tide_levels/tide_2014.csv')\n",
    "tide_2015_newport = pd.read_csv('./data/newport/tide_levels/tide_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tide(df):\n",
    "    df = df.drop(columns= ['Predicted (ft)', 'Preliminary (ft)'], axis=1).set_index('Date')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df[df['Verified (ft)'] != '-']\n",
    "    df['Verified (ft)'] = pd.to_numeric(df['Verified (ft)'])\n",
    "    df = df.resample('D').mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tide_2010_atlantic = clean_tide(tide_2010_atlantic)\n",
    "tide_2011_atlantic = clean_tide(tide_2011_atlantic)\n",
    "tide_2012_atlantic = clean_tide(tide_2012_atlantic)\n",
    "tide_2013_atlantic = clean_tide(tide_2013_atlantic)\n",
    "tide_2014_atlantic = clean_tide(tide_2014_atlantic)\n",
    "tide_2015_atlantic = clean_tide(tide_2015_atlantic)\n",
    "tide_2010_boston = clean_tide(tide_2010_boston)\n",
    "tide_2011_boston = clean_tide(tide_2011_boston)\n",
    "tide_2012_boston = clean_tide(tide_2012_boston)\n",
    "tide_2013_boston = clean_tide(tide_2013_boston)\n",
    "tide_2014_boston = clean_tide(tide_2014_boston)\n",
    "tide_2015_boston = clean_tide(tide_2015_boston)\n",
    "tide_2010_bridgeport = clean_tide(tide_2010_bridgeport)\n",
    "tide_2011_bridgeport = clean_tide(tide_2011_bridgeport)\n",
    "tide_2012_bridgeport = clean_tide(tide_2012_bridgeport)\n",
    "tide_2013_bridgeport = clean_tide(tide_2013_bridgeport)\n",
    "tide_2014_bridgeport = clean_tide(tide_2014_bridgeport)\n",
    "tide_2015_bridgeport = clean_tide(tide_2015_bridgeport)\n",
    "tide_2010_new_haven = clean_tide(tide_2010_new_haven)\n",
    "tide_2011_new_haven = clean_tide(tide_2011_new_haven)\n",
    "tide_2012_new_haven = clean_tide(tide_2012_new_haven)\n",
    "tide_2013_new_haven = clean_tide(tide_2013_new_haven)\n",
    "tide_2014_new_haven = clean_tide(tide_2014_new_haven)\n",
    "tide_2015_new_haven = clean_tide(tide_2015_new_haven)\n",
    "tide_2010_new_london = clean_tide(tide_2010_new_london)\n",
    "tide_2011_new_london = clean_tide(tide_2011_new_london)\n",
    "tide_2012_new_london = clean_tide(tide_2012_new_london)\n",
    "tide_2013_new_london = clean_tide(tide_2013_new_london)\n",
    "tide_2014_new_london = clean_tide(tide_2014_new_london)\n",
    "tide_2015_new_london = clean_tide(tide_2015_new_london)\n",
    "tide_2010_newport = clean_tide(tide_2010_newport)\n",
    "tide_2011_newport = clean_tide(tide_2011_newport)\n",
    "tide_2012_newport = clean_tide(tide_2012_newport)\n",
    "tide_2013_newport = clean_tide(tide_2013_newport)\n",
    "tide_2014_newport = clean_tide(tide_2014_newport)\n",
    "tide_2015_newport = clean_tide(tide_2015_newport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_tide = pd.concat([tide_2010_atlantic, tide_2011_atlantic, tide_2012_atlantic, tide_2013_atlantic, tide_2014_atlantic, tide_2015_atlantic])\n",
    "boston_tide = pd.concat([tide_2010_boston, tide_2011_boston, tide_2012_boston, tide_2013_boston, tide_2014_boston, tide_2015_boston])\n",
    "bridgeport_tide = pd.concat([tide_2010_bridgeport, tide_2011_bridgeport, tide_2012_bridgeport, tide_2013_bridgeport, tide_2014_bridgeport, tide_2015_bridgeport])\n",
    "new_haven_tide = pd.concat([tide_2010_new_haven, tide_2011_new_haven, tide_2012_new_haven, tide_2013_new_haven, tide_2014_new_haven, tide_2015_new_haven])\n",
    "new_london_tide = pd.concat([tide_2010_new_london, tide_2011_new_london, tide_2012_new_london, tide_2013_new_london, tide_2014_new_london, tide_2015_new_london])\n",
    "newport_tide = pd.concat([tide_2010_newport, tide_2011_newport, tide_2012_newport, tide_2013_newport, tide_2014_newport, tide_2015_newport])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "baro_2010_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2010.csv')\n",
    "baro_2011_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2011.csv')\n",
    "baro_2012_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2012.csv')\n",
    "baro_2013_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2013.csv')\n",
    "baro_2014_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2014.csv')\n",
    "baro_2015_atlantic = pd.read_csv('./data/Atlantic_city_data/baro_pressure/baro_2015.csv')\n",
    "baro_2010_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2010.csv')\n",
    "baro_2011_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2011.csv')\n",
    "baro_2012_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2012.csv')\n",
    "baro_2013_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2013.csv')\n",
    "baro_2014_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2014.csv')\n",
    "baro_2015_boston = pd.read_csv('./data/Boston/baro_pressure/baro_2015.csv')\n",
    "baro_2010_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2010.csv')\n",
    "baro_2011_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2011.csv')\n",
    "baro_2012_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2012.csv')\n",
    "baro_2013_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2013.csv')\n",
    "baro_2014_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2014.csv')\n",
    "baro_2015_bridgeport = pd.read_csv('./data/bridgeport_data/baro_pressure/baro_2015.csv')\n",
    "baro_2010_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2010.csv')\n",
    "baro_2011_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2011.csv')\n",
    "baro_2012_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2012.csv')\n",
    "baro_2013_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2013.csv')\n",
    "baro_2014_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2014.csv')\n",
    "baro_2015_new_haven = pd.read_csv('./data/new_haven_data/baro_pressure/baro_2015.csv')\n",
    "baro_2010_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2010.csv')\n",
    "baro_2011_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2011.csv')\n",
    "baro_2012_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2012.csv')\n",
    "baro_2013_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2013.csv')\n",
    "baro_2014_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2014.csv')\n",
    "baro_2015_new_london = pd.read_csv('./data/new_london_data/baro_pressure/baro_2015.csv')\n",
    "baro_2010_newport = pd.read_csv('./data/newport/baro_pressure/baro_2010.csv')\n",
    "baro_2011_newport = pd.read_csv('./data/newport/baro_pressure/baro_2011.csv')\n",
    "baro_2012_newport = pd.read_csv('./data/newport/baro_pressure/baro_2012.csv')\n",
    "baro_2013_newport = pd.read_csv('./data/newport/baro_pressure/baro_2013.csv')\n",
    "baro_2014_newport = pd.read_csv('./data/newport/baro_pressure/baro_2014.csv')\n",
    "baro_2015_newport = pd.read_csv('./data/newport/baro_pressure/baro_2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_baro(df):\n",
    "    df['DATE TIME'] = df['DATE TIME'].map(lambda x: x[:10])\n",
    "    df = df[['DATE TIME', ' BARO']]\n",
    "    df.rename(columns={'DATE TIME': 'Date', ' BARO': 'baro'}, inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.set_index('Date')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df['baro'] = pd.to_numeric(df['baro'])\n",
    "    df = df.resample('D').mean()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "baro_2010_atlantic = clean_baro(baro_2010_atlantic)\n",
    "baro_2011_atlantic = clean_baro(baro_2011_atlantic)\n",
    "baro_2012_atlantic = clean_baro(baro_2012_atlantic)\n",
    "baro_2013_atlantic = clean_baro(baro_2013_atlantic)\n",
    "baro_2014_atlantic = clean_baro(baro_2014_atlantic)\n",
    "baro_2015_atlantic = clean_baro(baro_2015_atlantic)\n",
    "baro_2010_boston = clean_baro(baro_2010_boston)\n",
    "baro_2011_boston = clean_baro(baro_2011_boston)\n",
    "baro_2012_boston = clean_baro(baro_2012_boston)\n",
    "baro_2013_boston = clean_baro(baro_2013_boston)\n",
    "baro_2014_boston = clean_baro(baro_2014_boston)\n",
    "baro_2015_boston = clean_baro(baro_2015_boston)\n",
    "baro_2010_bridgeport = clean_baro(baro_2010_bridgeport)\n",
    "baro_2011_bridgeport = clean_baro(baro_2011_bridgeport)\n",
    "baro_2012_bridgeport = clean_baro(baro_2012_bridgeport)\n",
    "baro_2013_bridgeport = clean_baro(baro_2013_bridgeport)\n",
    "baro_2014_bridgeport = clean_baro(baro_2014_bridgeport)\n",
    "baro_2015_bridgeport = clean_baro(baro_2015_bridgeport)\n",
    "baro_2010_new_haven = clean_baro(baro_2010_new_haven)\n",
    "baro_2011_new_haven = clean_baro(baro_2011_new_haven)\n",
    "baro_2012_new_haven = clean_baro(baro_2012_new_haven)\n",
    "baro_2013_new_haven = clean_baro(baro_2013_new_haven)\n",
    "baro_2014_new_haven = clean_baro(baro_2014_new_haven)\n",
    "baro_2015_new_haven = clean_baro(baro_2015_new_haven)\n",
    "baro_2010_new_london = clean_baro(baro_2010_new_london)\n",
    "baro_2011_new_london = clean_baro(baro_2011_new_london)\n",
    "baro_2012_new_london = clean_baro(baro_2012_new_london)\n",
    "baro_2013_new_london = clean_baro(baro_2013_new_london)\n",
    "baro_2014_new_london = clean_baro(baro_2014_new_london)\n",
    "baro_2015_new_london = clean_baro(baro_2015_new_london)\n",
    "baro_2010_newport = clean_baro(baro_2010_newport)\n",
    "baro_2011_newport = clean_baro(baro_2011_newport)\n",
    "baro_2012_newport = clean_baro(baro_2012_newport)\n",
    "baro_2013_newport = clean_baro(baro_2013_newport)\n",
    "baro_2014_newport = clean_baro(baro_2014_newport)\n",
    "baro_2015_newport = clean_baro(baro_2015_newport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_baro = pd.concat([baro_2010_atlantic, baro_2011_atlantic, baro_2012_atlantic, baro_2013_atlantic, baro_2014_atlantic, baro_2015_atlantic])\n",
    "boston_baro = pd.concat([baro_2010_boston, baro_2011_boston, baro_2012_boston, baro_2013_boston, baro_2014_boston, baro_2015_boston])\n",
    "bridgeport_baro = pd.concat([baro_2010_bridgeport, baro_2011_bridgeport, baro_2012_bridgeport, baro_2013_bridgeport, baro_2014_bridgeport, baro_2015_bridgeport])\n",
    "new_haven_baro = pd.concat([baro_2010_new_haven, baro_2011_new_haven, baro_2012_new_haven, baro_2013_new_haven, baro_2014_new_haven, baro_2015_new_haven])\n",
    "new_london_baro = pd.concat([baro_2010_new_london, baro_2011_new_london, baro_2012_new_london, baro_2013_new_london, baro_2014_new_london, baro_2015_new_london])\n",
    "newport_baro = pd.concat([baro_2010_newport, baro_2011_newport, baro_2012_newport, baro_2013_newport, baro_2014_newport, baro_2015_newport])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic = pd.merge(atlantic_tide, atlantic_baro, on='Date')\n",
    "boston = pd.merge(boston_tide, boston_baro, on='Date')\n",
    "bridgeport = pd.merge(bridgeport_tide, bridgeport_baro, on='Date')\n",
    "new_haven = pd.merge(new_haven_tide, new_haven_baro, on='Date')\n",
    "new_london = pd.merge(new_london_tide, new_london_baro, on='Date')\n",
    "newport = pd.merge(newport_tide, newport_baro, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_unemploy = pd.read_csv('./data/Atlantic_city_data/unemployment_nonseaonal.csv')\n",
    "boston_unemploy = pd.read_csv('./data/Boston/unemployment_nonseasonal.csv')\n",
    "bridgeport_unemploy = pd.read_csv('./data/bridgeport_data/unemployment_nonseasonal.csv')\n",
    "new_haven_unemploy = pd.read_csv('./data/new_haven_data/unemployment_nonseasonal.csv')\n",
    "new_london_unemploy = pd.read_csv('./data/new_london_data/unemployment_nonseasonal.csv')\n",
    "newport_unemploy = pd.read_csv('./data/newport/unemployment_nonseasonal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_unemploy(df, noaa_data):\n",
    "    df.columns = ['Date', 'unemployment_rate']\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df.set_index('Date')\n",
    "    df['unemployment_rate'] = pd.to_numeric(df['unemployment_rate'])\n",
    "    list_days = noaa_data.index\n",
    "    rate_list = [df.loc[str(i)[:7]]['unemployment_rate'].item() for i in list_days]\n",
    "    unemploy_df = pd.DataFrame(data=rate_list, index=list_days)\n",
    "    unemploy_df.rename(columns={0: 'rate'}, inplace=True)\n",
    "    return unemploy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_unemploy_df = clean_unemploy(atlantic_unemploy, atlantic)\n",
    "boston_unemploy_df = clean_unemploy(boston_unemploy, boston)\n",
    "bridgeport_unemploy_df = clean_unemploy(bridgeport_unemploy, bridgeport)\n",
    "new_haven_unemploy_df = clean_unemploy(new_haven_unemploy, new_haven)\n",
    "new_london_unemploy_df = clean_unemploy(new_london_unemploy, new_london)\n",
    "newport_unemploy_df = clean_unemploy(newport_unemploy, newport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlantic_city = pd.merge(atlantic, atlantic_unemploy_df, on='Date')\n",
    "boston = pd.merge(boston, boston_unemploy_df, on='Date')\n",
    "bridgeport = pd.merge(bridgeport, bridgeport_unemploy_df, on='Date')\n",
    "new_haven = pd.merge(new_haven, new_haven_unemploy_df, on='Date')\n",
    "new_london = pd.merge(new_london, new_london_unemploy_df, on='Date')\n",
    "newport = pd.merge(newport, newport_unemploy_df, on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verified (ft)</th>\n",
       "      <th>baro</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-01</th>\n",
       "      <td>-0.073333</td>\n",
       "      <td>1012.895238</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-02</th>\n",
       "      <td>0.557500</td>\n",
       "      <td>1005.982609</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>-0.865000</td>\n",
       "      <td>1003.373913</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-04</th>\n",
       "      <td>0.077500</td>\n",
       "      <td>1005.636364</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-05</th>\n",
       "      <td>0.332500</td>\n",
       "      <td>1007.321053</td>\n",
       "      <td>13.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Verified (ft)         baro  rate\n",
       "Date                                        \n",
       "2010-01-01      -0.073333  1012.895238  13.9\n",
       "2010-01-02       0.557500  1005.982609  13.9\n",
       "2010-01-03      -0.865000  1003.373913  13.9\n",
       "2010-01-04       0.077500  1005.636364  13.9\n",
       "2010-01-05       0.332500  1007.321053  13.9"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlantic_city.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
